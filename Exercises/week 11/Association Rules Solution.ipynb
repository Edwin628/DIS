{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Association Rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today, we will implement two techniques that are part of the so-called shopping basket analysis, which will help us to better understand how customers data are being processed to extract insights about their habits.\n",
    "\n",
    "\n",
    "#### Notes about external libraries\n",
    "You can check your implementation of the Apriori algorithm and the Association Rules using MLxtend, a data mining library. Unfortunately, the library is not directly shipped with Anaconda. To install MLxtend, just execute  \n",
    "\n",
    "```bash\n",
    "pip install mlxtend  \n",
    "```\n",
    "\n",
    "Or directly using Anaconda\n",
    "\n",
    "```bash\n",
    "conda install -c conda-forge mlxtend \n",
    "```\n",
    "\n",
    "Note that the installation of MLxtend is not mandatory, as we will provide the expected results in pre-rendered cells.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📚 Exercise 1: Apriori algorithm\n",
    "In the first excercise, we will put into practice the Apriori algorithm. In particular, we will extract frequent itemsets from a list of transactions coming from a grocery store. You will have to complete the function `get_support(...)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import operator\n",
    "import numpy as np\n",
    "\n",
    "\"\"\"\n",
    "Format the transaction dataset.\n",
    "Expect a list of transaction in the format:\n",
    "[[1, 3, 4], [2, 3, 5], [1, 2, 3, 5], ...]\n",
    "\"\"\"\n",
    "def preprocess(dataset):\n",
    "    unique_items = set()\n",
    "    for transaction in dataset:\n",
    "        for item in transaction:\n",
    "            unique_items.add(item)\n",
    "       \n",
    "    # Converting to frozensets to use itemsets as dict key\n",
    "    unique_items = [frozenset([i]) for i in list(unique_items)]\n",
    "    \n",
    "    return unique_items,list(map(set,dataset))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Generate candidates of length n+1 from a list of items, each of length n.\n",
    "\n",
    "Example:\n",
    "[{1}, {2}, {5}]          -> [{1, 2}, {1, 5}, {2, 5}]\n",
    "[{2, 3}, {2, 5}, {3, 5}] -> [{2, 3, 5}]\n",
    "\"\"\"\n",
    "def generate_candidates(Lk):\n",
    "    output = []\n",
    "\n",
    "    # We generate rules of the target size k\n",
    "    k=len(Lk[0])+1\n",
    "    \n",
    "    for i in range(len(Lk)):\n",
    "        for j in range(i+1, len(Lk)): \n",
    "            L1 = list(Lk[i])[:k-2]; \n",
    "            L2 = list(Lk[j])[:k-2]\n",
    "            L1.sort(); \n",
    "            L2.sort()\n",
    "\n",
    "            # Merge sets if first k-2 elements are equal\n",
    "            # For the case of k<2, generate all possible combinations\n",
    "            if L1==L2: \n",
    "                output.append(Lk[i] | Lk[j])\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Print the results of the apriori algorithm\n",
    "\"\"\"\n",
    "def print_support(support,max_display=10,min_items=1):\n",
    "    print('support\\t itemset')\n",
    "    print('-'*30)\n",
    "    filt_support = {k:v for k,v in support.items() if len(k)>=min_items}\n",
    "    for s,sup in sorted(filt_support.items(), key=operator.itemgetter(1),reverse=True)[:max_display]:\n",
    "        print(\"%.2f\" % sup,'\\t',set(s))\n",
    "        \n",
    "def print_support_mx(df,max_display=10,min_items=1):\n",
    "    print('support\\t itemset')\n",
    "    print('-'*30)\n",
    "    lenrow = df['itemsets'].apply(lambda x: len(x))\n",
    "    df  = df[lenrow>=min_items]\n",
    "    df  = df.sort_values('support',ascending=False).iloc[:max_display]\n",
    "    for i,row in df.iterrows():\n",
    "        print(\"%.2f\" % float(row['support']),'\\t',set(row['itemsets']))\n",
    "        \n",
    "\n",
    "\"\"\"\n",
    "Run the apriori algorithm\n",
    "\n",
    "dataset     : list of transactions\n",
    "min_support : minimum support. Itemsets with support below this threshold\n",
    "              will be pruned.\n",
    "\"\"\"\n",
    "def apriori(dataset, min_support = 0.5):\n",
    "    unique_items,dataset = preprocess(dataset)\n",
    "    L1, supportData      = get_support(dataset, unique_items, min_support)\n",
    "    \n",
    "    L = [L1]\n",
    "    k = 0\n",
    "    while True:\n",
    "        Ck       = generate_candidates(L[k])\n",
    "        Lk, supK = get_support(dataset, Ck, min_support)\n",
    "        \n",
    "        # Is there itemsets of length k that have the minimum support ?\n",
    "        if len(Lk)>0:\n",
    "            supportData.update(supK)\n",
    "            L.append(Lk) \n",
    "            k += 1\n",
    "        else:\n",
    "            break\n",
    "            \n",
    "    return L, supportData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TODO\n",
    "\n",
    "Compute support for all the candidate itemsets contained in Ck, given the total list of transactions. We already provide the functions to compute candidate itemsets. The support of the itemset $X$ with respect to the list of transactions $T$ is defined as the proportion of transactions $t$ in the dataset which contains the itemset $X$. Support can be computed using the following formula\n",
    "\n",
    "$$\\mathrm{supp}(X) = \\frac{|\\{t \\in T; X \\subseteq t\\}|}{|T|}$$  \n",
    "\n",
    "After computing the support for each itemset, prune the ones that do not match the minimal specificied support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Compute support for each provided itemset by counting the number of\n",
    "its occurences in the original dataset of transactions.\n",
    "\n",
    "dataset      : list of transactions, preprocessed using 'preprocess()'\n",
    "Ck           : list of itemsets to compute support for. \n",
    "min_support  : minimum support. Itemsets with support below this threshold\n",
    "               will be pruned.\n",
    "              \n",
    "output       : list of remaining itemsets, after the pruning step.\n",
    "support_dict : dictionary containing the support value for each itemset.\n",
    "\"\"\"\n",
    "def get_support(dataset, Ck, min_support):\n",
    "    support_count = {}\n",
    "    for transaction in dataset:\n",
    "        for candidate in Ck:\n",
    "            if candidate.issubset(transaction):\n",
    "                if not candidate in support_count: support_count[candidate]=1\n",
    "                else: support_count[candidate] += 1\n",
    "                    \n",
    "    num_transactions = float(len(dataset))\n",
    "    output = []\n",
    "    support_dict = {}\n",
    "    for key in support_count:\n",
    "        support = support_count[key]/num_transactions\n",
    "        if support >= min_support:\n",
    "            output.insert(0,key)\n",
    "            support_dict[key] = support\n",
    "    return output, support_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "support\t itemset\n",
      "------------------------------\n",
      "0.07 \t {'other vegetables', 'whole milk'}\n",
      "0.06 \t {'rolls/buns', 'whole milk'}\n",
      "0.06 \t {'whole milk', 'yogurt'}\n",
      "0.05 \t {'whole milk', 'root vegetables'}\n",
      "0.05 \t {'other vegetables', 'root vegetables'}\n",
      "0.04 \t {'other vegetables', 'yogurt'}\n",
      "0.04 \t {'rolls/buns', 'other vegetables'}\n",
      "0.04 \t {'whole milk', 'tropical fruit'}\n",
      "0.04 \t {'whole milk', 'soda'}\n",
      "0.04 \t {'rolls/buns', 'soda'}\n"
     ]
    }
   ],
   "source": [
    "dataset = [ l.strip().split(',') for i,l in enumerate(open('groceries.csv').readlines())]\n",
    "\n",
    "L,support = apriori(dataset,min_support=0.01)\n",
    "print_support(support,10,min_items=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qh/45t3t2ws29984shqtr0d7wgh0000gn/T/ipykernel_15042/3951043272.py:4: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.sum(level=1) should use df.groupby(level=1).sum().\n",
      "  df_dummy = pd.get_dummies(pd.Series(dataset).apply(pd.Series).stack()).sum(level=0)\n",
      "/opt/anaconda3/envs/adaexam/lib/python3.8/site-packages/mlxtend/frequent_patterns/fpcommon.py:111: DeprecationWarning: DataFrames with non-bool types result in worse computationalperformance and their support might be discontinued in the future.Please use a DataFrame with bool type\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "support\t itemset\n",
      "------------------------------\n",
      "0.07 \t {'other vegetables', 'whole milk'}\n",
      "0.06 \t {'rolls/buns', 'whole milk'}\n",
      "0.06 \t {'whole milk', 'yogurt'}\n",
      "0.05 \t {'whole milk', 'root vegetables'}\n",
      "0.05 \t {'other vegetables', 'root vegetables'}\n",
      "0.04 \t {'other vegetables', 'yogurt'}\n",
      "0.04 \t {'rolls/buns', 'other vegetables'}\n",
      "0.04 \t {'tropical fruit', 'whole milk'}\n",
      "0.04 \t {'whole milk', 'soda'}\n",
      "0.04 \t {'rolls/buns', 'soda'}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from mlxtend.frequent_patterns import apriori as mx_apriori\n",
    "\n",
    "df_dummy = pd.get_dummies(pd.Series(dataset).apply(pd.Series).stack()).sum(level=0)\n",
    "frequent_itemsets = mx_apriori(df_dummy, min_support=0.01, use_colnames=True)\n",
    "print_support_mx(frequent_itemsets,10,min_items=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📚 Exercise 2: Association Rule Learning\n",
    "Such associations are not necessarily symmetric. Therefore, in the second part, we will use [association rule learning](https://en.wikipedia.org/wiki/Association_rule_learning) to better understand the directionality of our computed frequent itemsets. In other terms, we will have to infer if the purchase of one item generally implies the the purchase of another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "L              : itemsets\n",
    "supportData    : dictionary storing itemsets support\n",
    "min_confidence : rules with a confidence under this threshold should be pruned\n",
    "\"\"\"\n",
    "def generate_rules(L, supportData, min_confidence=0.7):  \n",
    "    # Rules to be computed\n",
    "    rules = []\n",
    "    \n",
    "    # Iterate over itemsets of length 2..N\n",
    "    for i in range(1, len(L)):\n",
    "        \n",
    "        # Iterate over each frequent itemset\n",
    "        for freqSet in L[i]:\n",
    "            H1 = [frozenset([item]) for item in freqSet]\n",
    "            \n",
    "            # If the itemset contains more than 2 elements\n",
    "            # recursively generate candidates \n",
    "            if (i+1 > 2):\n",
    "                rules_from_consequent(freqSet, H1, supportData, rules, min_confidence)\n",
    "                compute_confidence(freqSet, H1, supportData, rules, min_confidence)\n",
    "            # If the itemsset contains 2 or less elements\n",
    "            # conpute rule confidence\n",
    "            else:\n",
    "                compute_confidence(freqSet, H1, supportData, rules, min_confidence)\n",
    "\n",
    "    return rules   \n",
    "\n",
    "\"\"\"\n",
    "freqSet        : frequent itemset\n",
    "H              : candidate elements to create a rule\n",
    "supportData    : dictionary storing itemsets support\n",
    "rules          : array to store rules\n",
    "min_confidence : rules with a confidence under this threshold should be pruned\n",
    "\"\"\"\n",
    "def rules_from_consequent(freqSet, H, supportData, rules, min_confidence=0.7):\n",
    "    m = len(H[0])\n",
    "    if (len(freqSet) > (m + 1)): \n",
    "\n",
    "        # create new candidates of size n+1\n",
    "        Hmp1 = generate_candidates(H)\n",
    "        Hmp1 = compute_confidence(freqSet, Hmp1, supportData, rules, min_confidence)\n",
    "        \n",
    "        if (len(Hmp1) > 1):    #need at least two sets to merge\n",
    "            rules_from_consequent(freqSet, Hmp1, supportData, rules, min_confidence)\n",
    "            \n",
    "\"\"\"\n",
    "Print the resulting rules\n",
    "\"\"\"\n",
    "def print_rules(rules,max_display=10):\n",
    "    print('confidence\\t rule')\n",
    "    print('-'*30)\n",
    "    for a,b,sup in sorted(rules, key=lambda x: x[2],reverse=True)[:max_display]:\n",
    "        print(\"%.2f\" % sup,'\\t',set(a),'->',set(b))\n",
    "def print_rules_mx(df,max_display=10):\n",
    "    print('confidence\\t rule')\n",
    "    print('-'*30)\n",
    "    df  = df.sort_values('confidence',ascending=False).iloc[:max_display]\n",
    "    for i,row in df.iterrows():\n",
    "        print(\"%.2f\" % float(row['confidence']),'\\t',set(row['antecedents']),'->',set(row['consequents']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TODO:\n",
    "\n",
    "You will have to complete the method `compute_confidence(...)`, that computes confidence for a set of candidate rules H and prunes the rules that have a confidence below the specified threshold. Please complete it by computing rules confidence using the following formula:\n",
    "\n",
    "$$\\mathrm{conf}(X \\Rightarrow Y) = \\mathrm{supp}(X \\cup Y) / \\mathrm{supp}(X)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Compute confidence for a given set of rules and their respective support\n",
    "\n",
    "freqSet        : frequent itemset of N-element\n",
    "H              : list of candidate elements Y1, Y2... that are part of the frequent itemset\n",
    "supportData    : dictionary storing itemsets support\n",
    "rules          : array to store rules\n",
    "min_confidence : rules with a confidence under this threshold should be pruned\n",
    "\"\"\"\n",
    "def compute_confidence(freqSet, H, supportData, rules, min_confidence=0.7):\n",
    "    prunedH = [] \n",
    "    \n",
    "    for Y in H:\n",
    "        X           = freqSet - Y\n",
    "        support_XuY = supportData[freqSet]\n",
    "        support_X   = supportData[X]\n",
    "        \n",
    "        conf        = support_XuY/support_X\n",
    "        \n",
    "        if conf >= min_confidence: \n",
    "            rules.append((X, Y, conf))\n",
    "            prunedH.append(Y)\n",
    "    return prunedH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confidence\t rule\n",
      "------------------------------\n",
      "0.59 \t {'citrus fruit', 'root vegetables'} -> {'other vegetables'}\n",
      "0.58 \t {'tropical fruit', 'root vegetables'} -> {'other vegetables'}\n",
      "0.58 \t {'yogurt', 'curd'} -> {'whole milk'}\n",
      "0.57 \t {'butter', 'other vegetables'} -> {'whole milk'}\n",
      "0.57 \t {'tropical fruit', 'root vegetables'} -> {'whole milk'}\n",
      "0.56 \t {'yogurt', 'root vegetables'} -> {'whole milk'}\n",
      "0.55 \t {'other vegetables', 'domestic eggs'} -> {'whole milk'}\n",
      "0.52 \t {'yogurt', 'whipped/sour cream'} -> {'whole milk'}\n",
      "0.52 \t {'rolls/buns', 'root vegetables'} -> {'whole milk'}\n",
      "0.52 \t {'other vegetables', 'pip fruit'} -> {'whole milk'}\n"
     ]
    }
   ],
   "source": [
    "rules=generate_rules(L,support, min_confidence=0.1)\n",
    "print_rules(rules,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confidence\t rule\n",
      "------------------------------\n",
      "0.59 \t {'citrus fruit', 'root vegetables'} -> {'other vegetables'}\n",
      "0.58 \t {'tropical fruit', 'root vegetables'} -> {'other vegetables'}\n",
      "0.58 \t {'yogurt', 'curd'} -> {'whole milk'}\n",
      "0.57 \t {'butter', 'other vegetables'} -> {'whole milk'}\n",
      "0.57 \t {'tropical fruit', 'root vegetables'} -> {'whole milk'}\n",
      "0.56 \t {'yogurt', 'root vegetables'} -> {'whole milk'}\n",
      "0.55 \t {'other vegetables', 'domestic eggs'} -> {'whole milk'}\n",
      "0.52 \t {'yogurt', 'whipped/sour cream'} -> {'whole milk'}\n",
      "0.52 \t {'rolls/buns', 'root vegetables'} -> {'whole milk'}\n",
      "0.52 \t {'other vegetables', 'pip fruit'} -> {'whole milk'}\n"
     ]
    }
   ],
   "source": [
    "from mlxtend.frequent_patterns import association_rules as mx_association_rules\n",
    "\n",
    "rules_mx = mx_association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.1)\n",
    "print_rules_mx(rules_mx,max_display=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EPFL Twitter Data\n",
    "\n",
    "Now that we have a working implementation, we will apply the Apriori algorithm on a dataset that you should know pretty well by now: EPFL Twitter data. In that scenario, tweets will be considered as transactions and words will be items. Let's see what kind of frequent associations we can discover.\n",
    "\n",
    "The method below cleans the tweets and formats them in the same format as the transactions of the previous exercise. Run the cells and generate the results for both algorithms. What can you observe from the association rules results? Briefly explain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/freened/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package punkt to /Users/freened/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Loading of libraries and documents\n",
    "\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import math\n",
    "from collections import Counter\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Tokenize, stem a document\n",
    "stemmer = PorterStemmer()\n",
    "def tokenize(text):\n",
    "    text = \"\".join([ch for ch in text if ch not in string.punctuation])\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return \" \".join([stemmer.stem(word.lower()) for word in tokens])\n",
    "\n",
    "# Remove stop words\n",
    "def clean_voc(documents):\n",
    "    cleaned = []\n",
    "    for tweet in documents:\n",
    "        new_tweet = []\n",
    "        tweet = tokenize(tweet).split()\n",
    "        for word in tweet:\n",
    "            if (word not in stopwords.words('english') and \n",
    "                word not in stopwords.words('german') and\n",
    "                word not in stopwords.words('french')):\n",
    "                if word==\"epflen\":\n",
    "                    word = \"epfl\"\n",
    "                new_tweet.append(word)\n",
    "        if len(new_tweet)>0:\n",
    "            cleaned.append(new_tweet)\n",
    "    return cleaned\n",
    "\n",
    "# Read a list of documents from a file. Each line in a file is a document\n",
    "with open(\"epfldocs.txt\") as f:\n",
    "    content = f.readlines()\n",
    "original_documents = [x.strip() for x in content] \n",
    "documents = clean_voc(original_documents)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2018 solution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Compute support for each provided itemset by counting the number of\n",
    "its occurences in the original dataset of transactions.\n",
    "\n",
    "dataset      : list of transactions, preprocessed using 'preprocess()'\n",
    "Ck           : list of itemsets to compute support for. \n",
    "min_support  : minimum support. Itemsets with support below this threshold\n",
    "               will be pruned.\n",
    "              \n",
    "output       : list of remaining itemsets, after the pruning step.\n",
    "support_dict : dictionary containing the support value for each itemset.\n",
    "\"\"\"\n",
    "def get_support(dataset, Ck, min_support):\n",
    "    support_count = {}\n",
    "    for transaction in dataset:\n",
    "        for candidate in Ck:\n",
    "            if candidate.issubset(transaction):\n",
    "                if not candidate in support_count: support_count[candidate]=1\n",
    "                else: support_count[candidate] += 1\n",
    "                    \n",
    "    num_transactions = float(len(dataset))\n",
    "    output = []\n",
    "    support_dict = {}\n",
    "    for key in support_count:\n",
    "        support = support_count[key]\n",
    "        if support >= min_support:\n",
    "            output.insert(0,key)\n",
    "            support_dict[key] = support\n",
    "    return output, support_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/aibin/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/aibin/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Loading of libraries and documents\n",
    "\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import math\n",
    "from collections import Counter\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Tokenize, stem a document\n",
    "stemmer = PorterStemmer()\n",
    "def tokenize(text):\n",
    "    text = \"\".join([ch for ch in text if ch not in string.punctuation])\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return \" \".join([stemmer.stem(word.lower()) for word in tokens])\n",
    "\n",
    "# Remove stop words\n",
    "def clean_voc(documents):\n",
    "    cleaned = []\n",
    "    for tweet in documents:\n",
    "        new_tweet = []\n",
    "        tweet = tokenize(tweet).split()\n",
    "        for word in tweet:\n",
    "            if (word not in stopwords.words('english') and \n",
    "                word not in stopwords.words('german') and\n",
    "                word not in stopwords.words('french')):\n",
    "                if word==\"epflen\":\n",
    "                    word = \"epfl\"\n",
    "                new_tweet.append(word)\n",
    "        if len(new_tweet)>0:\n",
    "            cleaned.append(new_tweet)\n",
    "    return cleaned\n",
    "\n",
    "# Read a list of documents from a file. Each line in a file is a document\n",
    "content = ['An apple is a fruit and good for health. (F)',\n",
    "    'Apple is a tech company. (T)',\n",
    "    'Tech companies like Apple and Facebook have big data centers (T)',\n",
    "    'For maintaining health eat one apple a day. (F)',\n",
    "    'A new Apple data center has been opened next to the one of Facebook. (T)',\n",
    "    'Apple has sold 1 million units in one day. (T)',\n",
    "    'Fruits, like apples and pears, are contaminated with pesticides.  (F)',\n",
    "    'A fruit salad contains apples, bananas and pears.  (F)',\n",
    "    'I saw a new apple recipe on Facebook.  (F)',\n",
    "    'Apple is increasingly processing health data. (T)']\n",
    "original_documents = [x.strip() for x in content] \n",
    "documents = clean_voc(original_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['appl', 'fruit', 'good', 'health', 'f'],\n",
       " ['appl', 'tech', 'compani'],\n",
       " ['tech', 'compani', 'like', 'appl', 'facebook', 'big', 'data', 'center'],\n",
       " ['maintain', 'health', 'eat', 'one', 'appl', 'day', 'f'],\n",
       " ['new', 'appl', 'data', 'center', 'ha', 'open', 'next', 'one', 'facebook'],\n",
       " ['appl', 'ha', 'sold', '1', 'million', 'unit', 'one', 'day'],\n",
       " ['fruit', 'like', 'appl', 'pear', 'contamin', 'pesticid', 'f'],\n",
       " ['fruit', 'salad', 'contain', 'appl', 'banana', 'pear', 'f'],\n",
       " ['saw', 'new', 'appl', 'recip', 'facebook', 'f'],\n",
       " ['appl', 'increasingli', 'process', 'health', 'data']]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 代码间隔线上面都是2018专用代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{frozenset({'appl'}): 10,\n",
       " frozenset({'fruit'}): 3,\n",
       " frozenset({'health'}): 3,\n",
       " frozenset({'f'}): 5,\n",
       " frozenset({'tech'}): 2,\n",
       " frozenset({'compani'}): 2,\n",
       " frozenset({'data'}): 3,\n",
       " frozenset({'facebook'}): 3,\n",
       " frozenset({'like'}): 2,\n",
       " frozenset({'center'}): 2,\n",
       " frozenset({'one'}): 3,\n",
       " frozenset({'day'}): 2,\n",
       " frozenset({'new'}): 2,\n",
       " frozenset({'ha'}): 2,\n",
       " frozenset({'pear'}): 2,\n",
       " frozenset({'f', 'health'}): 2,\n",
       " frozenset({'f', 'fruit'}): 3,\n",
       " frozenset({'appl', 'f'}): 5,\n",
       " frozenset({'appl', 'health'}): 3,\n",
       " frozenset({'appl', 'fruit'}): 3,\n",
       " frozenset({'compani', 'tech'}): 2,\n",
       " frozenset({'appl', 'compani'}): 2,\n",
       " frozenset({'appl', 'tech'}): 2,\n",
       " frozenset({'center', 'facebook'}): 2,\n",
       " frozenset({'center', 'data'}): 2,\n",
       " frozenset({'appl', 'center'}): 2,\n",
       " frozenset({'appl', 'like'}): 2,\n",
       " frozenset({'data', 'facebook'}): 2,\n",
       " frozenset({'appl', 'facebook'}): 3,\n",
       " frozenset({'appl', 'data'}): 3,\n",
       " frozenset({'day', 'one'}): 2,\n",
       " frozenset({'appl', 'day'}): 2,\n",
       " frozenset({'appl', 'one'}): 3,\n",
       " frozenset({'ha', 'one'}): 2,\n",
       " frozenset({'appl', 'ha'}): 2,\n",
       " frozenset({'facebook', 'new'}): 2,\n",
       " frozenset({'appl', 'new'}): 2,\n",
       " frozenset({'f', 'pear'}): 2,\n",
       " frozenset({'fruit', 'pear'}): 2,\n",
       " frozenset({'appl', 'pear'}): 2,\n",
       " frozenset({'appl', 'f', 'fruit'}): 3,\n",
       " frozenset({'appl', 'f', 'health'}): 2,\n",
       " frozenset({'appl', 'compani', 'tech'}): 2,\n",
       " frozenset({'appl', 'data', 'facebook'}): 2,\n",
       " frozenset({'appl', 'center', 'facebook'}): 2,\n",
       " frozenset({'center', 'data', 'facebook'}): 2,\n",
       " frozenset({'appl', 'center', 'data'}): 2,\n",
       " frozenset({'appl', 'day', 'one'}): 2,\n",
       " frozenset({'appl', 'facebook', 'new'}): 2,\n",
       " frozenset({'appl', 'ha', 'one'}): 2,\n",
       " frozenset({'appl', 'fruit', 'pear'}): 2,\n",
       " frozenset({'f', 'fruit', 'pear'}): 2,\n",
       " frozenset({'appl', 'f', 'pear'}): 2,\n",
       " frozenset({'appl', 'center', 'data', 'facebook'}): 2,\n",
       " frozenset({'appl', 'f', 'fruit', 'pear'}): 2}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "support\t itemset\n",
      "------------------------------\n",
      "5.00 \t {'f', 'appl'}\n",
      "3.00 \t {'fruit', 'f'}\n",
      "3.00 \t {'health', 'appl'}\n",
      "3.00 \t {'fruit', 'appl'}\n",
      "3.00 \t {'facebook', 'appl'}\n",
      "3.00 \t {'appl', 'data'}\n",
      "3.00 \t {'appl', 'one'}\n",
      "3.00 \t {'fruit', 'f', 'appl'}\n",
      "2.00 \t {'f', 'health'}\n",
      "2.00 \t {'compani', 'tech'}\n",
      "2.00 \t {'compani', 'appl'}\n",
      "2.00 \t {'appl', 'tech'}\n",
      "2.00 \t {'facebook', 'center'}\n",
      "2.00 \t {'center', 'data'}\n",
      "2.00 \t {'center', 'appl'}\n",
      "2.00 \t {'like', 'appl'}\n",
      "2.00 \t {'facebook', 'data'}\n",
      "2.00 \t {'one', 'day'}\n",
      "2.00 \t {'appl', 'day'}\n",
      "2.00 \t {'ha', 'one'}\n",
      "2.00 \t {'ha', 'appl'}\n",
      "2.00 \t {'new', 'facebook'}\n",
      "2.00 \t {'new', 'appl'}\n",
      "2.00 \t {'f', 'pear'}\n",
      "2.00 \t {'fruit', 'pear'}\n",
      "2.00 \t {'pear', 'appl'}\n",
      "2.00 \t {'f', 'health', 'appl'}\n",
      "2.00 \t {'tech', 'compani', 'appl'}\n",
      "2.00 \t {'data', 'facebook', 'appl'}\n",
      "2.00 \t {'facebook', 'center', 'appl'}\n"
     ]
    }
   ],
   "source": [
    "L,support = apriori(documents,min_support = 2)\n",
    "print_support(support,30,min_items=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confidence\t rule\n",
      "------------------------------\n",
      "1.00 \t {'pear'} -> {'appl'}\n",
      "1.00 \t {'pear'} -> {'fruit'}\n",
      "1.00 \t {'pear'} -> {'f'}\n",
      "1.00 \t {'new'} -> {'appl'}\n",
      "1.00 \t {'new'} -> {'facebook'}\n",
      "1.00 \t {'ha'} -> {'appl'}\n",
      "1.00 \t {'ha'} -> {'one'}\n",
      "1.00 \t {'one'} -> {'appl'}\n",
      "1.00 \t {'day'} -> {'appl'}\n",
      "1.00 \t {'day'} -> {'one'}\n",
      "1.00 \t {'data'} -> {'appl'}\n",
      "1.00 \t {'facebook'} -> {'appl'}\n",
      "1.00 \t {'like'} -> {'appl'}\n",
      "1.00 \t {'center'} -> {'appl'}\n",
      "1.00 \t {'center'} -> {'data'}\n",
      "1.00 \t {'center'} -> {'facebook'}\n",
      "1.00 \t {'tech'} -> {'appl'}\n",
      "1.00 \t {'compani'} -> {'appl'}\n",
      "1.00 \t {'tech'} -> {'compani'}\n",
      "1.00 \t {'compani'} -> {'tech'}\n"
     ]
    }
   ],
   "source": [
    "rules=generate_rules(L,support, min_confidence=0.7)\n",
    "print_rules(rules,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📚 Exercise 3: Pen and Paper!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are given the following accident and weather data. Each line corresponds to one event:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. car_accident rain lightning wind clouds fire\n",
    "2. fire clouds rain lightning wind\n",
    "3. car_accident fire wind\n",
    "4. clouds rain wind\n",
    "5. lightning fire rain clouds 6. clouds wind car_accident 7. rain lightning clouds fire 8. lightning fire car_accident"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) You would like to know what is the likely cause of all the car accidents. What association rules do you need to look for? Compute the confidence and support values for these rules. Looking at these values, which is the most likely cause of the car accidents?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Find all the association rules for minimal support 0.6 and minimal confidence of 1.0 (certainty). Follow the apriori algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Solution\n",
    "We need to look for the association rules of the form: {cause} → {car accident}\n",
    "i.e. in which the left-hand side represents the cause of the accident. \n",
    "\n",
    "The possible association rules are: \n",
    "{lightning} → {car accident} support: 0.25 confidence: 0.4\n",
    "{wind} → {car accident} support: 0.375 confidence: 0.6\n",
    "{fire} → {car accident} support: 0.375 confidence: 0.5\n",
    "{clouds} → {car accident} support: 0.25 confidence: 0.33\n",
    "{rain} → {car accident} support: 0.125 confidence: 0.2\n",
    "\n",
    "{wind}has both the highest confidence and the highest support and is the most likely cause of the car accidents.\n",
    "\n",
    "\n",
    "We first find all the frequent itemsets of size one. The minimal support requirement is 0.6,\n",
    "which means that to be frequent an itemset must occur in at least 5 out of the 8 transactions, 5/8 = 0.\n",
    "6.25≥0.6. There are five frequent itemsets:{clouds} support: 0.75\n",
    "{wind} support: 0.625\n",
    "{lightning} support: 0.625\n",
    "{rain} support: 0.625\n",
    "{fire} support: 0.75\n",
    "From the above itemsets we next generate all possible itemsets of size 2 and prune the itemsets with support below 0.6. Only two itemsets remain:\n",
    "{lightning, fire} support: 0.625\n",
    "{clouds, rain} support: 0.625\n",
    "It is not possible to generate the itemsets of size 3 out of the above 2 itemsets, the intersection is empty. Based on the itemsets of size 2 we generate all possible association rules and compute their confidence:\n",
    " {lightning} →{fire} support: 0.625 confidence: 1.0\n",
    "{fire} → {lightning} support: 0.625 confidence: 0.833\n",
    "{clouds} → {rain} support: 0.625 confidence: 0.833\n",
    "{rain} → {clouds} support: 0.625 confidence: 1.0\n",
    "There are only two association rules with confidence equal to 1 and that is the final solution.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adaexam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "774b128520448664c0ce54185c50d2053e1a9ce2d7add6cdc69ac19bdaf04756"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
