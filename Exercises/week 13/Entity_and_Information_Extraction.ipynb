{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö Exercise 13: Entity & Relation Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Relation extraction from Wikipedia articles\n",
    "\n",
    "Use Wikipedia to extract the relation `directedBy(Movie, Person)` by applying pattern based heuristics that utilize: *Part Of Speech Tagging*, *Named Entity Recognition* and *Regular Expressions*.\n",
    "\n",
    "#### Required Library: SpaCy\n",
    "- ```conda install -y spacy```\n",
    "- ```python -m spacy download en```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request, json, csv, re\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read tsv with input movies\n",
    "def read_tsv():\n",
    "    movies=[]\n",
    "    with open('movies.tsv','r') as file:\n",
    "        tsv = csv.reader(file, delimiter='\\t')\n",
    "        next(tsv) #remove header\n",
    "        movies = [{'movie':line[0], 'director':line[1]} for line in tsv]\n",
    "    return movies\n",
    "\n",
    "#parse wikipedia page\n",
    "def parse_wikipedia(movie):\n",
    "    txt = ''\n",
    "    try:\n",
    "        with urllib.request.urlopen('https://en.wikipedia.org/w/api.php?format=json&action=query&prop=extracts&exintro=&explaintext=&titles='+movie) as url:\n",
    "            data = json.loads(url.read().decode())\n",
    "            txt = next (iter (data['query']['pages'].values()))['extract']\n",
    "    except:\n",
    "        pass\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Daniel Lee',\n",
       " 'Donnie Yen',\n",
       " 'Zhao Wei',\n",
       " 'Sammo Hung',\n",
       " 'Wu Chun',\n",
       " 'Kate Tsui',\n",
       " 'Qi Yuwu']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = parse_wikipedia('14_Blades')\n",
    "find_PER_entities(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Parse the raw text of a Wikipedia movie page and extract named (PER) entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def find_PER_entities(txt):\n",
    "\n",
    "    persons = []\n",
    "    doc = nlp(txt)\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == 'PERSON':\n",
    "            persons.append(ent.text)\n",
    "    \n",
    "    return persons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Given the raw text of a Wikipedia movie page and the extracted PER entities, find the director."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code re.sub('[!?,.]', '', txt) uses the re.sub() function from the re (regular expression) module to remove certain characters from a string txt.\n",
    "\n",
    "The first argument '[!?,.]' is a regular expression pattern that matches any of the characters !, ?, ,, or ..\n",
    "\n",
    "The second argument '' is a string that specifies what to replace the matched characters with. In this case, it's an empty string, so the characters will be removed.\n",
    "\n",
    "The third argument txt is the input string to perform the substitution on.\n",
    "\n",
    "So, the code will replace any instances of the characters !, ?, ,, or . in txt with an empty string, effectively removing those characters from txt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#simple heuristic: find the next PER entity after the word 'directed'\n",
    "\n",
    "def find_director(txt, persons):\n",
    "    txt_list = re.sub('[!?,.]','',txt).split() # Â∞ÜÊúâÈó¥ÈöîÁöÑÂêçÂ≠ó‰πüÂàÜÈöîÂºÄÂá∫Êù•‰∫Ü\n",
    "    for i in range(len(txt_list)):\n",
    "        if txt_list[i] == 'directed':\n",
    "            for j in range(i,len(txt_list)):\n",
    "                for per in persons:\n",
    "                    if per.startswith(txt_list[j]): #ÂøÖÈ°ªË¶ÅÁî®satrtwithÈÄöËøápersonÊù•ÊâæÔºåÂõ†‰∏∫personÊòØ‰∏≠Èó¥ÊúâÁ©∫Ê†ºÁöÑËøîÂõûÂÄº\n",
    "                        return per\n",
    "    return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'movie': '13_Assassins_(2010_film)', 'director': 'Takashi Miike'},\n",
       " {'movie': '14_Blades', 'director': 'Daniel Lee'},\n",
       " {'movie': '22_Bullets', 'director': 'Richard Berry'},\n",
       " {'movie': 'The_A-Team_(film)', 'director': 'Joe Carnahan'},\n",
       " {'movie': 'Alien_vs_Ninja', 'director': 'Seiji Chiba'},\n",
       " {'movie': 'Bad_Blood_(2010_film)', 'director': 'Dennis Law'},\n",
       " {'movie': 'Bangkok_Knockout', 'director': 'Panna Rittikrai'},\n",
       " {'movie': 'Blades_of_Blood', 'director': 'Lee Joon-ik'},\n",
       " {'movie': 'The_Book_of_Eli', 'director': 'Allen Hughes'},\n",
       " {'movie': 'The_Bounty_Hunter_(2010_film)', 'director': 'Andy Tennant'}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies = read_tsv()\n",
    "movies[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "statements=[]\n",
    "tp = 0\n",
    "for m in movies:\n",
    "\n",
    "        txt = parse_wikipedia(m['movie'])\n",
    "        persons = find_PER_entities(txt)\n",
    "        director = find_director(txt, persons)\n",
    "        \n",
    "        if director != '':\n",
    "            statements.append(m['movie'] + ' is directed by ' + director + '.')\n",
    "            if director == m['director']:\n",
    "                tp += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) Compute the precision and recall based on the given ground truth (column Director from tsv file) and show examples of statements that are extracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13_Assassins_(2010_film) is directed by J≈´sannin no Shikaku.\n",
      "22_Bullets is directed by L'Immortel.\n",
      "Alien_vs_Ninja is directed by Seiji Chiba.\n",
      "Blades_of_Blood is directed by Gureumeul Beoseonan Dalcheoreom.\n",
      "The_Bounty_Hunter_(2010_film) is directed by Andy Tennant.\n"
     ]
    }
   ],
   "source": [
    "for s in statements[:5]:\n",
    "    print (s)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.7763713080168776\n",
      "Recall: 0.6411149825783972\n",
      "\n",
      "***Sample Statements***\n",
      "['13_Assassins_(2010_film) is directed by Takashi Miike.', '14_Blades is directed by Daniel Lee.', '22_Bullets is directed by Richard Berry.', 'Alien_vs_Ninja is directed by Seiji Chiba.', 'Bad_Blood_(2010_film) is directed by Dennis Law.']\n"
     ]
    }
   ],
   "source": [
    "# compute precision and recall\n",
    "\n",
    "precision = tp / len(statements)\n",
    "recall = tp / len(movies)\n",
    "print ('Precision:',precision)\n",
    "print ('Recall:',recall)\n",
    "print('\\n***Sample Statements***')\n",
    "print(statements[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Named Entity Recognition using Hidden Markov Model\n",
    "\n",
    "\n",
    "Define a Hidden Markov Model (HMM) that recognizes Person (*PER*) entities.\n",
    "Particularly, your model must be able to recognize pairs of the form (*firstname lastname*) as *PER* entities.\n",
    "Using the given sentences as training and test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set=['The best blues singer was Bobby Bland while Ray Charles pioneered soul music .', \\\n",
    "              'Bobby Bland was just a singer whereas Ray Charles was a pianist , songwriter and singer .' \\\n",
    "              'None of them lived in Chicago .']\n",
    "\n",
    "test_set=['Ray Charles was born in 1930 .', \\\n",
    "          'Bobby Bland was born the same year as Ray Charles .', \\\n",
    "          'Muddy Waters is the father of Chicago Blues .']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Annotate your training set with the labels I (for PER entities) and O (for non PER entities).\n",
    "\t\n",
    "    *Hint*: Represent the sentences as sequences of bigrams, and label each bigram.\n",
    "\tOnly bigrams that contain pairs of the form (*firstname lastname*) are considered as *PER* entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annotation\n",
      " [['The best', 'O'], ['best blues', 'O'], ['blues singer', 'O'], ['singer was', 'O'], ['was Bobby', 'O'], ['Bobby Bland', 'I'], ['Bland while', 'O'], ['while Ray', 'O'], ['Ray Charles', 'I'], ['Charles pioneered', 'O'], ['pioneered soul', 'O'], ['soul music', 'O'], ['music .', 'O'], ['Bobby Bland', 'I'], ['Bland was', 'O'], ['was just', 'O'], ['just a', 'O'], ['a singer', 'O'], ['singer whereas', 'O'], ['whereas Ray', 'O'], ['Ray Charles', 'I'], ['Charles was', 'O'], ['was a', 'O'], ['a pianist', 'O'], ['pianist ,', 'O'], [', songwriter', 'O'], ['songwriter and', 'O'], ['and singer', 'O'], ['singer .None', 'O'], ['.None of', 'O'], ['of them', 'O'], ['them lived', 'O'], ['lived in', 'O'], ['in Chicago', 'O'], ['Chicago .', 'O']] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Bigram Representation\n",
    "def getBigrams(sents):\n",
    "    return [b[0]+' '+b[1] for l in sents for b in zip(l.split(' ')[:-1], l.split(' ')[1:])]\n",
    "\n",
    "bigrams = getBigrams(training_set)\n",
    "\n",
    "#Annotation\n",
    "PER = ['Bobby Bland', 'Ray Charles']\n",
    "annotations = []\n",
    "for b in bigrams:\n",
    "    if b in PER:\n",
    "        annotations.append([b,'I'])\n",
    "    else:\n",
    "        annotations.append([b,'O'])\n",
    "print('Annotation\\n', annotations,'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Compute the transition and emission probabilities for the HMM (use smoothing parameter $\\lambda$=0.5).\n",
    "\n",
    "    *Hint*: For the emission probabilities you can utilize the morphology of the words that constitute a bigram (e.g., you can count their uppercase first characters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_ = 0.5\n",
    "\n",
    "#Transition Probabilities\n",
    "transition_prob={}\n",
    "\n",
    "\n",
    "#Prior\n",
    "transition_prob['P(I|start)'] = ...\n",
    "transition_prob['P(O|start)'] = ...\n",
    "\n",
    "transition_prob['P(O|O)'] = ...\n",
    "transition_prob['P(O|I)'] = ...\n",
    "transition_prob['P(I|O)'] = ...\n",
    "transition_prob['P(I|I)'] = ...\n",
    "\n",
    "print('Transition Probabilities\\n',transition_prob, '\\n')\n",
    "\n",
    "#Emission Probabilities\n",
    "emission_prob={}\n",
    "\n",
    "        \n",
    "default_emission = ...\n",
    "\n",
    "emission_prob['P(2_upper|O)'] = ...\n",
    "emission_prob['P(2_upper|I)'] = ...\n",
    "emission_prob['P(1_upper|O)'] = ...\n",
    "emission_prob['P(1_upper|I)'] = ...\n",
    "emission_prob['P(0_upper|O)'] = ...\n",
    "emission_prob['P(0_upper|I)'] = ...\n",
    "\n",
    "print('Emission Probabilities\\n', emission_prob, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) Predict the labels of the test set and compute the precision and the recall of your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prediction\n",
    "bigrams = getBigrams(test_set)\n",
    "entities=[]\n",
    "prev_state='start'\n",
    "for b in bigrams:\n",
    "    I_prob = ...\n",
    "    O_prob = ...\n",
    "    \n",
    "    if ...:\n",
    "        prev_state = 'O'\n",
    "    else:\n",
    "        entities.append(b)\n",
    "        prev_state = 'I'\n",
    "\n",
    "print('Predicted Entities\\n', entities, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision is *...%* while recall is *...%*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4) Comment on how you can further improve this model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adaexam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "228px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  },
  "vscode": {
   "interpreter": {
    "hash": "774b128520448664c0ce54185c50d2053e1a9ce2d7add6cdc69ac19bdaf04756"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
