{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö Exercise 1\n",
    "In this exercise, we will understand the functioning of TF/IDF ranking and implement and test a vector space retrieval model.\n",
    "\n",
    "### Use case:\n",
    "You are an engineer who is building a search engine for the recipe finder application _bestbakesinthebuilding.com_. In this application, users can search for their favorite recipe, ingredients, or any recipe-relevant term. The search engine should return relevant recipes that are stored in the website's DB. For the implementation of the searching functionality, you have to use the vector space retrieval model.\n",
    "\n",
    "To build your system, you have available recipe data in the file bread.txt which is a dump from the application's DB.\n",
    "\n",
    "The schema of the data is the following:\n",
    "\n",
    "  DocID | Document Text\n",
    "  ------|------------------\n",
    "  1     | How to Bake Breads Without Baking Recipes\n",
    "  2     | Smith Pies: Best Pies in London\n",
    "  3     | Numerical Recipes: The Art of Scientific Computing\n",
    "  4     | Breads, Pastries, Pies, and Cakes: Quantity Baking Recipes\n",
    "  5     | Pastry: A Book of Best French Pastry Recipes\n",
    "\n",
    "\n",
    "### Goal:\n",
    "1. Preprocess the input text by tokenizing it and stemming it.\n",
    "2. Vectorize the preprocessed text by implementing the TF/IDF function.\n",
    "3. Test your method by searching for the query $Q = ``baking''$ and find the top-ranked documents according to the TF/IDF rank. \n",
    "4. Compare your implementation with the existing implementation of scikit-learn library.\n",
    "\n",
    "### What are you learning in this exercise:\n",
    "- Vector space retrieval model implementation.\n",
    "- TF/IDF ranking.\n",
    "- Evaluation of a text retrieval system.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/aibin/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/aibin/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading of libraries and documents\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import math\n",
    "from collections import Counter\n",
    "from operator import itemgetter\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Preprocess the input text by tokenizing it and stemming it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer() \n",
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    It tokenizes and stems an input text.\n",
    "    \n",
    "    :param text: str, with the input text\n",
    "    :return: list, of the tokenized and stemmed tokens.\n",
    "    \"\"\"\n",
    "    text = \"\".join([ch for ch in text if ch not in string.punctuation])\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return [stemmer.stem(word.lower()) for word in tokens if word not in stopwords.words('english')]\n",
    "\n",
    "# Read a list of documents from a file. Each line in a file is a document\n",
    "with open(\"epfldocs.txt\") as f:\n",
    "    content = f.readlines()\n",
    "original_documents = [x.strip() for x in content]\n",
    "documents = [tokenize(d) for d in original_documents]\n",
    "\n",
    "# documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['welcome', 'to']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = \"Numerical Recipes: The Art of Scientific Computing\"\n",
    "test_punc = \"\".join([ch for ch in test if ch not in string.punctuation])\n",
    "test_punc\n",
    "\n",
    "documents\n",
    "test_vocab = [vocab for sentence in documents for vocab in sentence]\n",
    "test_vocab\n",
    "\n",
    "len(documents)\n",
    "\n",
    "\n",
    "test_term = 'recip'\n",
    "test_term_doc = [test_term in document for document in documents]\n",
    "test_term_doc_num = sum([test_term in document for document in documents])\n",
    "test_term_doc\n",
    "\n",
    "test_vector = [0]*6\n",
    "test_vector\n",
    "\n",
    "test_counts = Counter(documents[0])\n",
    "max_count = test_counts.most_common(1)[0]\n",
    "test_counts\n",
    "\n",
    "# vector[i] =  sum([term in document]) * idf[term] / max_count  FALSE FALSE FALSE FALSE FALSE\n",
    "sum(['bake' in documents[0]])\n",
    "[term == 'bake' for term in documents[0]]\n",
    "\n",
    "txt = \"welcome to the jungle\"\n",
    "txt_split = txt.split() # cannot remove punctuation\n",
    "txt_split[:2] # exclude the last one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Vectorize the preprocessed text by implementing the TF/IDF function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for the TF/IDF implementation\n",
    "\n",
    "# create the vocabulary\n",
    "vocabulary = list(set([item for sublist in documents for item in sublist]))\n",
    "vocabulary.sort()\n",
    "\n",
    "def idf_values(vocabulary, documents):\n",
    "    \"\"\"\n",
    "    It computes IDF scores, storing idf values in a dictionary.\n",
    "    \n",
    "    :param vocabulary: list of str, with the unique tokens of the vocabulary.\n",
    "    :param documents: list of lists of str, with tokenized sentences.\n",
    "    :return: dict with the idf values for each vocabulary word.\n",
    "    \"\"\"\n",
    "    idf = {}\n",
    "    num_documents = len(documents)\n",
    "    for i, term in enumerate(vocabulary):\n",
    "        term_doc_num = sum([term in document for document in documents])\n",
    "        idf[term] = math.log(num_documents/term_doc_num)\n",
    "    return idf\n",
    "\n",
    "def vectorize(document, vocabulary, idf):\n",
    "    \"\"\"\n",
    "    It generates the vector for an input document (with normalization).\n",
    "    \n",
    "    :param document: list of str with the tokenized documents.\n",
    "    :param vocabulary: list of str, with the unique tokens of the vocabulary.\n",
    "    :param idf: dict with the idf values for each vocabulary word.\n",
    "    :return: list of floats\n",
    "    \"\"\"\n",
    "    vector = [0]*len(vocabulary)\n",
    "    counts = Counter(document)\n",
    "    max_count = counts.most_common(1)[0][1]\n",
    "    for i,term in enumerate(vocabulary):\n",
    "        # YOUR CODE HERE\n",
    "        vector[i] =  counts[term] * idf[term] / max_count\n",
    "    return vector\n",
    "\n",
    "# Compute IDF values and vectors\n",
    "idf = idf_values(vocabulary, documents)\n",
    "document_vectors = [vectorize(s, vocabulary, idf) for s in documents]\n",
    "\n",
    "def cosine_similarity(v1,v2):\n",
    "    \"\"\"\n",
    "    It computes cosine similarity.\n",
    "    \n",
    "    :param v1: list of floats, with the vector of a document.\n",
    "    :param v2: list of floats, with the vector of a document.\n",
    "    :return: float\n",
    "    \"\"\"\n",
    "    \n",
    "    sumxx, sumxy, sumyy = 0, 0, 0\n",
    "    for i in range(len(v1)):\n",
    "        x = v1[i]; y = v2[i]\n",
    "        sumxx += x*x\n",
    "        sumyy += y*y\n",
    "        sumxy += x*y\n",
    "    if sumxy == 0:\n",
    "            result = 0\n",
    "    else:\n",
    "        # YOUR CODE HERE\n",
    "            result = sumxy / math.sqrt(sumxx * sumyy)\n",
    "    return result\n",
    "\n",
    "def search_vec(query, topk):\n",
    "    \"\"\"\n",
    "    It computes the search result (get the topk documents).\n",
    "    \n",
    "    :param query: str\n",
    "    :param topk: int\n",
    "    \"\"\"\n",
    "    q = query.split()\n",
    "    q = [stemmer.stem(w) for w in q]\n",
    "    query_vector = vectorize(q, vocabulary, idf)\n",
    "    scores = [[cosine_similarity(query_vector, document_vectors[d]), d] for d in range(len(documents))]\n",
    "    scores.sort(key=lambda x: -x[0])\n",
    "    for i in range(topk):\n",
    "        print(original_documents[scores[i][1]])\n",
    "\n",
    "# HINTS\n",
    "# natural logarithm function\n",
    "#     math.log(n,math.e)\n",
    "# Function to count term frequencies in a document\n",
    "#     Counter(document)\n",
    "# most common elements for a list\n",
    "#     counts.most_common(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "range(0, 1075)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "range(len(documents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Test your method by searching for the query  ùëÑ=‚Äò‚Äòùëèùëéùëòùëñùëõùëî‚Ä≥  and find the top-ranked documents according to the TF/IDF rank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How to Bake Breads Without Baking Recipes\n",
      "Breads, Pastries, Pies, and Cakes: Quantity Baking Recipes\n",
      "Pastry: A Book of Best French Pastry Recipes\n",
      "Numerical Recipes: The Art of Scientific Computing\n",
      "Smith Pies: Best Pies in London\n"
     ]
    }
   ],
   "source": [
    "# Reference code using scikit-learn\n",
    "tf = TfidfVectorizer(analyzer='word', ngram_range=(1,1), min_df = 1, stop_words = 'english')\n",
    "features = tf.fit_transform(original_documents)\n",
    "npm_tfidf = features.todense()\n",
    "new_features = tf.transform(['baking'])\n",
    "\n",
    "cosine_similarities = linear_kernel(new_features, features).flatten()\n",
    "related_docs_indices = cosine_similarities.argsort()[::-1]\n",
    "topk = 5\n",
    "for i in range(topk):\n",
    "    print(original_documents[related_docs_indices[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How to Bake Breads Without Baking Recipes\n",
      "Breads, Pastries, Pies, and Cakes: Quantity Baking Recipes\n",
      "Smith Pies: Best Pies in London\n",
      "Numerical Recipes: The Art of Scientific Computing\n",
      "Pastry: A Book of Best French Pastry Recipes\n"
     ]
    }
   ],
   "source": [
    "topk = 5\n",
    "search_vec('baking', topk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.Compare your implementation with the existing implementation of scikit-learn library.\n",
    "\n",
    "In this part, we consider the scikit reference code as an ‚Äúoracle‚Äù that supposedly gives the correct result. Your task is to compare your implemented tf-idf retrieval model with this oracle for the following queries \"computer science\", \"IC school\", \"information systems\" on the **epfldocs.txt** collection.\n",
    "\n",
    "For this exercise, you need to replace **bread.txt** in the first cell with **epfldocs.txt** and rerun all the cells from the begining. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieval oracle \n",
    "tf = TfidfVectorizer(analyzer='word', ngram_range=(1,1), min_df = 1, stop_words = 'english')\n",
    "features = tf.fit_transform(original_documents)\n",
    "npm_tfidf = features.todense()\n",
    "\n",
    "def search_vec_sklearn(query, features, threshold=0.1):\n",
    "    \"\"\"\n",
    "    Return all document ids that that have cosine similarity with the query larger than a threshold.\n",
    "    \n",
    "    :param query: str\n",
    "    :param features: ndarray\n",
    "    :param threshold: float\n",
    "    :return: list of int\n",
    "    \"\"\"\n",
    "    new_features = tf.transform([query])\n",
    "    cosine_similarities = linear_kernel(new_features, features).flatten()\n",
    "    related_docs_indices, cos_sim_sorted = zip(*sorted(enumerate(cosine_similarities), key=itemgetter(1), \n",
    "                                                       reverse=True))\n",
    "    doc_ids = []\n",
    "    for i, cos_sim in enumerate(cos_sim_sorted):\n",
    "        if cos_sim < threshold:\n",
    "            break\n",
    "        doc_ids.append(related_docs_indices[i])\n",
    "    return doc_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exciting News: \"World University Rankings 2016-2017 by subject: computer science\" No1 @ETH &amp; @EPFL on No8. Congrats https://t.co/ARSlXZoShQ\n",
      "New computer model shows how proteins are controlled \"at a distance\" https://t.co/zNjK3bZ6mO  via @EPFL_en #VDtech https://t.co/b9TglXO4KD\n",
      "An interview with Patrick Barth, a new @EPFL professor who combines protein #biophysics with computer modeling https://t.co/iJwBaEbocj\n",
      "Exposure Science Film Hackathon 2017 applications open! Come join our Scicomm-film-hacking event! #Science #scicomm https://t.co/zwtKPlh6HT\n",
      "Le myst√®re Soulages √©blouit la science @EPFL  https://t.co/u3uNICyAdi\n",
      "@cwarwarrior @EPFL_en @EPFL Doing science at @EPFL_en is indeed pretty cool!!! Thank you for visiting!!!\n",
      "Blue Brain Nexus: an open-source tool for data-driven science https://t.co/m5yTgXf7ym #epfl\n",
      "Swiss Data Science on Twitter: \"Sign up for @EPFL_en #DataJamDays: learn more a‚Ä¶ https://t.co/kNVILHWPGb, see more https://t.co/2wg3BbHBNq\n",
      "The registration for Exposure Science Film Hackathon 2017 is open! #scicomm #lausanne #epfl #unil https://t.co/mY5jlwsXUD\n",
      "Know someone who has promoted sound #science? Less than 2 weeks to nominate them for the #MaddoxPrize. https://t.co/POnZtf3vFT\n",
      "#sleep #neuroscience #Neurosciences #science Healthy mitochondria could stop Alzheimer's https://t.co/rioZv4axxN #epfl\n",
      "Its about Energy &amp; People ! Blue Brain Nexus: an open-source tool for data-driven science https://t.co/1kTFeYqZe6 #epfl\n",
      "Blue Brain Nexus: an open-source tool for data-driven science https://t.co/Hou5tl7RoJ  via @EPFL_en #VDtech https://t.co/eftNaVCNh6\n",
      "New report on risk of misuse of life science research https://t.co/6HaoSkJTmZ #epfl  @ScnatCH https://t.co/VmkwW0Q2Gq\n",
      "The internet is amplifying the popularity of irrational food fads. Time for some science, says @EPFL_en https://t.co/MPPlAaHJ3X\n",
      "Deep Learning on Graphs, the Christmas lecture of @EPFL_en's course A Network Tour of Data Science. https://t.co/0Ke069OMBn https://t.co/o84K23WBLm\n",
      "Today I visited my friend @wonderqueens at @EPFL_en who showed me around how unbelievably cool is this place for doing science üòç Thanks!! https://t.co/kWDnn0UXMN\n",
      "Hey #ASHG17 trainees, looking for a postdoc in genomics? Come to Switzerland: lake, mountains, exciting science: https://t.co/JaQq6zQKdC\n",
      "Noir c'est noir? Les Outrenoirs de Pierre Soulages | CULTURE/ART/SCIENCE https://t.co/6ZTZov8ylH #epfl #epflcampus\n",
      "Blue Brain Nexus: An open-source knowledge graph for data-driven science https://t.co/XcrufBSg5a via @EPFL_en @EurekAlert @BlueBrainPjt @wake_sleep\n",
      "Data Science and Mobility Conference is co-organized by EPFL and SBB CFF FFS. 31 JANV 2018 https://t.co/PNAm80x8Tn - @EPFL https://t.co/4ldolDo6UW\n",
      "ETH Z√ºrich und EPFL haben das Swiss Data Science Center er√∂ffnet @ETH @EPFL_en @SDSCdatascience https://t.co/PdoHEAo9w6 https://t.co/OMaseBuL3L\n",
      "Interessanter Artikel von @MirkoBischBerg @EPFL aus Texas. #virtualreality #science #arts. Evolution der Maschinen https://t.co/dNOt4OimmV\n",
      "@MartinVetterli @EPFL setzt sich f√ºr open science ein. Wissen teilen f√ºr mehr Wirkung. https://t.co/4hMAZtA9Qa\n",
      "\"Art and science collisions\" - A presentation of art residence @CERN with Yunchul Kim and Helga Timko at @EPFL ----&gt; https://t.co/GT73OAEVOH\n",
      "Je r√™ve! m√™me la science et l'innovation sont touch√©es par ce d√©cret! Moyen-Age 2.0 #Trump #NoBanNoWall @EPFL https://t.co/gYcLIE4vHL @tdgch\n"
     ]
    }
   ],
   "source": [
    "ret_ids = search_vec_sklearn('computer science', features)\n",
    "for i, v in enumerate(ret_ids):\n",
    "    print(original_documents[v])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modified version\n",
    "def search_vec(query, topk):\n",
    "    \"\"\"\n",
    "    It computes the search result (get the topk documents).\n",
    "    \n",
    "    :param query: str\n",
    "    :param topk: int\n",
    "    \"\"\"\n",
    "    q = query.split()\n",
    "    q = [stemmer.stem(w) for w in q]\n",
    "    query_vector = vectorize(q, vocabulary, idf)\n",
    "    scores = [[cosine_similarity(query_vector, document_vectors[d]), d] for d in range(len(documents))]\n",
    "    scores.sort(key=lambda x: -x[0])\n",
    "    for i in range(topk):\n",
    "        print(original_documents[scores[i][1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exciting News: \"World University Rankings 2016-2017 by subject: computer science\" No1 @ETH &amp; @EPFL on No8. Congrats https://t.co/ARSlXZoShQ\n",
      "New computer model shows how proteins are controlled \"at a distance\" https://t.co/zNjK3bZ6mO  via @EPFL_en #VDtech https://t.co/b9TglXO4KD\n",
      "An interview with Patrick Barth, a new @EPFL professor who combines protein #biophysics with computer modeling https://t.co/iJwBaEbocj\n",
      "Video of Nicola Marzari from @EPFL_en  on Computational Discovery in the 21st Century during #PASC17 now online: https://t.co/tfCkEvYKtq https://t.co/httPdHcK9W\n",
      "New at @epfl_en Life Sciences @epflSV: \"From PhD directly to Independent Group Leader\" #ELFIR_EPFL:  Early Independence Research Scholars. See https://t.co/evqyqD7FFl, also for computational biology #compbio https://t.co/e3pDCg6NVb Deadline April 1 2018 at https://t.co/mJqcrfIqkb\n",
      "@CodeWeekEU is turning 5, yay! We look very much forward to computational thinking unplugged activities during @CodeWeek_CH https://t.co/yDPrlKg4hw\n",
      "Exposure Science Film Hackathon 2017 applications open! Come join our Scicomm-film-hacking event! #Science #scicomm https://t.co/zwtKPlh6HT\n",
      "Le myst√®re Soulages √©blouit la science @EPFL  https://t.co/u3uNICyAdi\n",
      "Blue Brain Nexus: an open-source tool for data-driven science https://t.co/m5yTgXf7ym #epfl\n",
      "@cwarwarrior @EPFL_en @EPFL Doing science at @EPFL_en is indeed pretty cool!!! Thank you for visiting!!!\n"
     ]
    }
   ],
   "source": [
    "search_vec('computer science', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [\"computer science\", \"IC school\", \"information systems\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_recall_at_k(predict, gt, k):\n",
    "    \"\"\"\n",
    "    It computes the recall score at a defined set of retrieved documents.\n",
    "    \n",
    "    :param predict: list of predictions\n",
    "    :param gt: list of actual data\n",
    "    :param k: int\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    correct = set(predict[:k]).intersection(set(gt))\n",
    "    return len(correct) / len(gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_precision_at_k(predict, gt, k):\n",
    "    \"\"\"\n",
    "    It computes the precision score at a defined set of retrieved documents.\n",
    "    \n",
    "    :param predict: list of predictions\n",
    "    :param gt: list of actual data\n",
    "    :param k: int\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    correct = set(predict[:k]).intersection(set(gt))\n",
    "    return len(correct) / k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_map(queries, K = 10):\n",
    "    \"\"\"\n",
    "    It computes mean average precision (MAP) for a set of queries.\n",
    "    \n",
    "    :param queries: list of str\n",
    "    :param K: int\n",
    "    \"\"\"\n",
    "    map_score = 0\n",
    "    for i, query in enumerate(queries):\n",
    "        precision_for_query = 0\n",
    "        predict = search_vec(query, K)\n",
    "        gt = search_vec_sklearn(query, features)\n",
    "        for k in range(1, K+1):\n",
    "            # YOUR CODE HERE\n",
    "        # YOUR CODE HERE\n",
    "        map_score += \n",
    "    # YOUR CODE HERE\n",
    "    map_score = map_score / len(queries) \n",
    "    return map_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_map(queries)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adaexam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "774b128520448664c0ce54185c50d2053e1a9ce2d7add6cdc69ac19bdaf04756"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
