{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed Information Systems\n",
    "\n",
    "***Final Exam, Spring Semester, 2020***\n",
    "\n",
    "The exam will be held on your computer, but digital communication by any means is **strictly prohibited**. You are allowed, though, to use *StackOverflow* or similar websites to resolve syntax-related Python errors. \n",
    "The following materials are also allowed: exercise sheets and solutions, past exams with your own solution, personally written notes and personally collected documentation.\n",
    "By participating in this exam you **agree to these conditions**.\n",
    "\n",
    "These are the instructions for the exam:\n",
    "\n",
    "- You are not allowed to leave the examination room in the first 20 and the last 15 minutes of the exam.\n",
    "- The quiz will remain open **only for the first 2 hours** of the exam to avoid network congestion.\n",
    "- **30 minutes** before the end of the exam we will announce a password to upload your jupyter notebook on Moodle.\n",
    "- It is not recommended to leave the exam before the password is published. If you need to leave earlier, contact us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#0-Rename-your-Notebook\" data-toc-modified-id=\"0-Rename-your-Notebook-0\">0 Rename your Notebook</a></span></li><li><span><a href=\"#1-Multiple-Choice-Questions\" data-toc-modified-id=\"1-Multiple-Choice-Questions-1\">1 <a href=\"https://moodle.epfl.ch/mod/quiz/view.php?id=1026316\" target=\"_blank\">Multiple Choice Questions</a></a></span></li><li><span><a href=\"#2-Implementing-a-Rule-Based-Approach-for-Entity-Disambiguation\" data-toc-modified-id=\"2-Implementing-a-Rule-Based-Approach-for-Entity-Disambiguation-2\">2 Implementing a Rule-Based Approach for Entity Disambiguation</a></span><ul class=\"toc-item\"><li><span><a href=\"#2.1-Learning-rules\" data-toc-modified-id=\"2.1-Learning-rules-2.1\">2.1 Learning rules</a></span></li><li><span><a href=\"#2.2-Finding-new-rules-using-bootstrapping\" data-toc-modified-id=\"2.2-Finding-new-rules-using-bootstrapping-2.2\">2.2 Finding new rules using bootstrapping</a></span></li></ul></li><li><span><a href=\"#3-Academic-Communities\" data-toc-modified-id=\"3-Academic-Communities-3\">3 Academic Communities</a></span><ul class=\"toc-item\"><li><span><a href=\"#3.1-Modularity\" data-toc-modified-id=\"3.1-Modularity-3.1\">3.1 Modularity</a></span></li><li><span><a href=\"#3.2-Community-Detection\" data-toc-modified-id=\"3.2-Community-Detection-3.2\">3.2 Community Detection</a></span></li><li><span><a href=\"#3.3-Community-Influencers\" data-toc-modified-id=\"3.3-Community-Influencers-3.3\">3.3 Community Influencers</a></span></li><li><span><a href=\"#3.4-Connectivity-Based-Community-Ranking\" data-toc-modified-id=\"3.4-Connectivity-Based-Community-Ranking-3.4\">3.4 Connectivity-Based Community Ranking</a></span></li><li><span><a href=\"#3.5-Personalized-Community-Ranking\" data-toc-modified-id=\"3.5-Personalized-Community-Ranking-3.5\">3.5 Personalized Community Ranking</a></span></li><li><span><a href=\"#3.6-TF-IDF-Community-Ranking\" data-toc-modified-id=\"3.6-TF-IDF-Community-Ranking-3.6\">3.6 TF-IDF Community Ranking</a></span></li><li><span><a href=\"#3.7-Rankings-Correlation\" data-toc-modified-id=\"3.7-Rankings-Correlation-3.7\">3.7 Rankings Correlation</a></span></li></ul></li><li><span><a href=\"#4-Submit--your-Notebook\" data-toc-modified-id=\"4-Submit--your-Notebook-4\">4 <a href=\"https://moodle.epfl.ch/mod/quiz/view.php?id=1026302\" target=\"_blank\">Submit  your Notebook</a></a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0 Rename your Notebook\n",
    "Replace SciperNo with your **personal SCIPER Number**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 [Multiple Choice Questions](https://moodle.epfl.ch/mod/quiz/view.php?id=1026316)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Implementing a Rule-Based Approach for Entity Disambiguation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Learning rules\n",
    "\n",
    "We would like to develop a rule-based approach to disambiguate the meaning of the term \"Apple\" in sentences into the two possible meanings **Tech** and **Fruit**. The rules use keyphrases i.e., n-grams of words, as features in their conditions. In the approach **only the top-k keyphrases** are considered. Before forming keyphrases, the words are stemmed, stopwords are not considered and uppercase words are converted to lowercase.\n",
    "\n",
    "An example of a possible rule would be:\n",
    "\n",
    "\tif \"fruit\" and \"facebook\" then \"Tech\"\n",
    "\n",
    "meaning that if a sentence that contains the term \"Apple\" also contains the unigrams \"fruit\" and \"facebook\" then the meaning of \"Apple\" should be the Apple technology company (**Tech**).\n",
    "\n",
    "More generally we write such rules as:\n",
    "\n",
    "\t{k1,...,kn} -> Tech or {k1,...,kn} -> Fruit\n",
    "    \n",
    "where k1,..,kn are arbitrary word n-grams and a set {k1,...,kn} is considered as an item set.\n",
    "\n",
    "As **training data** we obtain 10 sentences containing \"Apple\" that have been labelled by their meaning as T(ech) or F(ruit).\n",
    "\n",
    "    An apple is a fruit and good for health. (F)\n",
    "    Apple is a tech company. (T)\n",
    "    Tech companies like Apple and Facebook have big data centers (T)\n",
    "    For maintaining health eat one apple a day. (F)\n",
    "    A new Apple data center has been opened next to the one of Facebook. (T)\n",
    "    Apple has sold 1 million units in one day. (T)\n",
    "    Fruits, like apples and pears, are contaminated with pesticides.  (F)\n",
    "    A fruit salad contains apples, bananas and pears.  (F)\n",
    "    I saw a new apple recipe on Facebook.  (F)\n",
    "    Apple is increasingly processing health data. (T)\n",
    "\n",
    "**Stopwords** are: an, is, a, and, for, like, and, have, has, been, in, are, with, on, by, as, may, one, to, the, of.\n",
    "\n",
    "We define the **confidence** of a rule {k1,...,kn} -> T as the fraction between the number of sentences that contain all keyphrases k1,...,kn with label T over the number of sentences with all keyphrases k1,...,kn. Similar definition for {k1,...,kn} -> F.\n",
    "\n",
    "We define the **support** of an itemset as the number of sentences that contain all the n-grams in the itemset.\n",
    "\n",
    "**Questions:** \n",
    "1. Without considering the word \"apple\", determine the keyphrases with minimum document frequency of 2.\n",
    "2. Determine all itemsets of keyphrases with a minimum support of 2.\n",
    "3. Determine all rules that have a minimum support of 2 and a minimum confidence of 70%. Provide for each rule its support and confidence values. \n",
    "4. Explain how the apriori property could be exploited in optimizing the computation of those itemsets for composite keyphrases.\n",
    "\n",
    "Hint: the following are the top-10 document frequencies of terms.\n",
    "\n",
    "| Term        | Frequency           | \n",
    "| ------------- |:-------------:| \n",
    "|fruit | 3|\n",
    "|health | 3|\n",
    "|data | 3|\n",
    "|facebook | 3|\n",
    "|tech | 2|\n",
    "|compani | 2|\n",
    "|center | 2|\n",
    "|day | 2|\n",
    "|new | 2|\n",
    "|pear |2|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***ANSWER:***\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Finding new rules using bootstrapping\n",
    "\n",
    "We are using the rules we have learnt now to find new sentences on Apple (e.g. using Google search) to increase our training data and thus learning new rules.\n",
    "\n",
    "Performing this approach we found 10 more sentences:\n",
    "\n",
    "    Apple and Facebook data center filed plans to expand data center operations in Prineville (T) \n",
    "    Apple, Facebook, Google Asked to Pay for Wind Parks in Denmark (T) \n",
    "    Google, Facebook and Apple lead on green data centers (T)\n",
    "    Apple to contest patent filed by Google. (T) \n",
    "    Green data centers are the new priority for Apple and Google. (T)\n",
    "    Green apples are an ideal ingredient for fruit salad. (F)\n",
    "    Apple Pears have been described as the hottest new item since the Kiwi (F)\n",
    "    Join Facebook to connect with Apples Pear and others you may know (F)\n",
    "    An apple is a sweet, edible fruit produced by an apple tree (F)\n",
    "    An apple a day keeps the doctor away (F)\n",
    "\n",
    "**Questions:** \n",
    "\n",
    "1. Given that the size of the training set has increased, should confidence and/or support threshold be adapted and how?\n",
    "2. Identify all new rules with adapted confidence and support threshold that can be derived from the training set that has been enlarged by the examples that have been retrieved using a bootstrapping approach.\n",
    "3. Once you found a new rule, explain how you would proceed to verify that the new rule does not introduce semantic shift."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***ANSWER:***\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Academic Communities\n",
    "The dataset you are about to explore is a snapshot of the **retweet network** among official accounts of universities and academic institutes. The **nodes** of the network are twitter handles (usernames), while the **edges** are attributed with a **label**, which was extracted from the topic of the tweet, and a **weight**, which depicts the popularity of the original tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from networkx.algorithms import community\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'uni_network.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m G \u001b[39m=\u001b[39m nx\u001b[39m.\u001b[39mfrom_pandas_edgelist(pd\u001b[39m.\u001b[39;49mread_csv(\u001b[39m'\u001b[39;49m\u001b[39muni_network.csv\u001b[39;49m\u001b[39m'\u001b[39;49m), \u001b[39m'\u001b[39m\u001b[39mSource\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mTarget\u001b[39m\u001b[39m'\u001b[39m, edge_attr\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mLabel\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mWeight\u001b[39m\u001b[39m'\u001b[39m])\n",
      "File \u001b[0;32m/opt/anaconda3/envs/DIS/lib/python3.10/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[39m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/DIS/lib/python3.10/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/DIS/lib/python3.10/site-packages/pandas/io/parsers/readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    946\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[1;32m    947\u001b[0m )\n\u001b[1;32m    948\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 950\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/DIS/lib/python3.10/site-packages/pandas/io/parsers/readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    602\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[1;32m    604\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 605\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    607\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[1;32m    608\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/opt/anaconda3/envs/DIS/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1439\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m   1441\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1442\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/DIS/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode:\n\u001b[1;32m   1734\u001b[0m         mode \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m-> 1735\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(\n\u001b[1;32m   1736\u001b[0m     f,\n\u001b[1;32m   1737\u001b[0m     mode,\n\u001b[1;32m   1738\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1739\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1740\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[1;32m   1741\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[1;32m   1742\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m   1743\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1744\u001b[0m )\n\u001b[1;32m   1745\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[0;32m/opt/anaconda3/envs/DIS/lib/python3.10/site-packages/pandas/io/common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    851\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    852\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    853\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    854\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[1;32m    855\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[0;32m--> 856\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[1;32m    857\u001b[0m             handle,\n\u001b[1;32m    858\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[1;32m    859\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[1;32m    860\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[1;32m    861\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    862\u001b[0m         )\n\u001b[1;32m    863\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    864\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[1;32m    865\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'uni_network.csv'"
     ]
    }
   ],
   "source": [
    "G = nx.from_pandas_edgelist(pd.read_csv('uni_network.csv'), 'Source', 'Target', edge_attr=['Label', 'Weight'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Modularity\n",
    "- Implement the modularity metric for communities.\n",
    "- Use the toy example and the assertion below to validate your results.\n",
    "- Hint: You can reuse code from the exercise sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def communities_modularity(G, nodes_community):\n",
    "    ''' input: G:nx.Graph \n",
    "               nodes_community:{node_id:community_id}\n",
    "       output: Q (modularity metric)\n",
    "    '''\n",
    "    Q = 0\n",
    "    \n",
    "    # Add your code here\n",
    "    m = len(G.edges)\n",
    "    for node_i in G.nodes:\n",
    "        for node_j in G.nodes:\n",
    "            if nodes_community[node_i] == nodes_community[node_j]:\n",
    "                Q += G.number_of_edges(node_i, node_j) - G.degree[node_i]*G.degree[node_j]/(2*m)\n",
    "    Q = Q/(2*m)\n",
    "    return Q \n",
    "\n",
    "Q = communities_modularity(nx.Graph([(1, 2), (2, 3), (3, 1)]), {1:'a', 2:'a', 3:'a'})\n",
    "assert (round(Q, 4) == 0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Community Detection\n",
    "\n",
    "- In the following cells you are given two snippets of code for community detection:\n",
    "    - The first approach (`network_structure_communities`) computes communities based on the **network structure**. The algorithm that is used is the *Clauset-Newman-Moore greedy modularity maximization* and is similar to the *Louvain modularity maximization*.\n",
    "    - The second approach (`nodes_label_communities`) computes communities based on the **labels of the graph nodes**. The label of a node is determined by the dominant label of its edges. All nodes with the same label belong to the same community.\n",
    "\n",
    "- Based on the statistics that we present below, discuss what are the **pros and cons** of each approach.\n",
    "- Note: You don't have to code for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def network_structure_communities(G):\n",
    "    nodes_community = community.greedy_modularity_communities(G, weight='Weight')\n",
    "    nodes_community = {i:indx for indx, c in enumerate(nodes_community) for i in c}\n",
    "    nodes_community = {n:nodes_community[n] for n in G.nodes}\n",
    "    return nodes_community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nodes_label_communities(G):\n",
    "    nodes_community = {}\n",
    "    for n in G.nodes():\n",
    "        labels = [[e[2]['Label']]*e[2]['Weight'] for e in G.edges(n, data=True)]\n",
    "        labels = [item for l in labels for item in l]\n",
    "        nodes_community[n] = max(set(labels), key = labels.count)\n",
    "    return nodes_community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'G' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 10\u001b[0m\n\u001b[1;32m      5\u001b[0m     communities_count_2 \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mset\u001b[39m(nodes_community_2\u001b[39m.\u001b[39mvalues()))\n\u001b[1;32m      7\u001b[0m     \u001b[39mreturn\u001b[39;00m pd\u001b[39m.\u001b[39mDataFrame\u001b[39m.\u001b[39mfrom_dict([{\u001b[39m'\u001b[39m\u001b[39m'\u001b[39m:\u001b[39m'\u001b[39m\u001b[39m1st approach\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mCommunities Count\u001b[39m\u001b[39m'\u001b[39m:communities_count_1, \u001b[39m'\u001b[39m\u001b[39mAverage Community Size\u001b[39m\u001b[39m'\u001b[39m:\u001b[39mlen\u001b[39m(G\u001b[39m.\u001b[39mnodes())\u001b[39m/\u001b[39mcommunities_count_1, \u001b[39m'\u001b[39m\u001b[39mModularity\u001b[39m\u001b[39m'\u001b[39m:communities_modularity(G, nodes_community_1)},\\\n\u001b[1;32m      8\u001b[0m                                    {\u001b[39m'\u001b[39m\u001b[39m'\u001b[39m:\u001b[39m'\u001b[39m\u001b[39m2nd approach\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mCommunities Count\u001b[39m\u001b[39m'\u001b[39m:communities_count_2, \u001b[39m'\u001b[39m\u001b[39mAverage Community Size\u001b[39m\u001b[39m'\u001b[39m:\u001b[39mlen\u001b[39m(G\u001b[39m.\u001b[39mnodes())\u001b[39m/\u001b[39mcommunities_count_2, \u001b[39m'\u001b[39m\u001b[39mModularity\u001b[39m\u001b[39m'\u001b[39m:communities_modularity(G, nodes_community_2)}])\u001b[39m.\u001b[39mset_index(\u001b[39m'\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m compare_approaches(G)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'G' is not defined"
     ]
    }
   ],
   "source": [
    "def compare_approaches(G):\n",
    "    nodes_community_1 = network_structure_communities(G)\n",
    "    nodes_community_2 = nodes_label_communities(G)\n",
    "    communities_count_1 = len(set(nodes_community_1.values()))\n",
    "    communities_count_2 = len(set(nodes_community_2.values()))\n",
    "\n",
    "    return pd.DataFrame.from_dict([{'':'1st approach', 'Communities Count':communities_count_1, 'Average Community Size':len(G.nodes())/communities_count_1, 'Modularity':communities_modularity(G, nodes_community_1)},\\\n",
    "                                   {'':'2nd approach', 'Communities Count':communities_count_2, 'Average Community Size':len(G.nodes())/communities_count_2, 'Modularity':communities_modularity(G, nodes_community_2)}]).set_index('')\n",
    "\n",
    "compare_approaches(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***ANSWER:***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following tasks we will use the communities that are detected by the **second approach**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_community = nodes_label_communities(G)\n",
    "communities = list(set(nodes_community.values()))\n",
    "communities_count = len(communities)\n",
    "default_ranking = {c:0.0 for c in communities}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Community Influencers\n",
    "- Isolate each community from the graph. \n",
    "- Select the node with the **maximum pagerank** within each community as the **influencer** of that community.\n",
    "- Break ties arbitrarily.\n",
    "- Hint: Useful functions: `nx.pagerank()`, `G.subgraph()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'G' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 14\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[39m# Add your code here\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     \u001b[39mreturn\u001b[39;00m influencers\n\u001b[0;32m---> 14\u001b[0m influencers \u001b[39m=\u001b[39m community_influencers(G, nodes_community, communities, communities_count)\n\u001b[1;32m     15\u001b[0m \u001b[39msorted\u001b[39m(influencers\u001b[39m.\u001b[39mitems(), key\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m x: x[\u001b[39m0\u001b[39m]) \u001b[39mif\u001b[39;00m influencers \u001b[39m!=\u001b[39m {} \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'G' is not defined"
     ]
    }
   ],
   "source": [
    "def community_influencers(G, nodes_community, communities, communities_count):\n",
    "    ''' input: G:nx.Graph \n",
    "               nodes_community:{node_id:community_id}\n",
    "               communities:[community_ids]\n",
    "               community_count:int\n",
    "       output: influencers:{community_id:node_id}\n",
    "    '''\n",
    "    influencers = {}\n",
    "    \n",
    "    # Add your code here\n",
    "    \n",
    "    return influencers\n",
    "\n",
    "influencers = community_influencers(G, nodes_community, communities, communities_count)\n",
    "sorted(influencers.items(), key=lambda x: x[0]) if influencers != {} else None # prints sorted results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Connectivity-Based Community Ranking\n",
    "- Compute a meta graph where nodes are communities and edges denote inter-connections across communities. \n",
    "- Add the weights of the inter-connections as weights to the edges.\n",
    "- Compute `pagerank` on the meta graph.\n",
    "- Hint: `w_matrix` is the confusion matrix of the weights among the communities. `w_matrix` is not symmetric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connectivity_ranking(G, nodes_community, communities, communities_count):\n",
    "    ''' input: G:nx.Graph \n",
    "               nodes_community:{node_id:community_id}\n",
    "               communities:[community_ids]\n",
    "               community_count:int\n",
    "       output: communities_ranking:{community_id:ranking}\n",
    "\n",
    "    '''\n",
    "    communities_ranking = default_ranking.copy()\n",
    "    \n",
    "    meta_G = nx.Graph()\n",
    "    w_matrix = {c2:{c1:0 for c1 in communities} for c2 in communities}\n",
    "    for (n1, n2, weight) in G.edges(data='Weight'):\n",
    "        w_matrix[nodes_community[n1]][nodes_community[n2]] += weight\n",
    "\n",
    "    # Add your code here\n",
    "        \n",
    "    return communities_ranking\n",
    "\n",
    "connectivity_ranking = connectivity_ranking(G, nodes_community, communities, communities_count)\n",
    "sorted(connectivity_ranking.items(), key=lambda x: x[1], reverse=True) if connectivity_ranking != default_ranking else None # prints sorted results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Personalized Community Ranking\n",
    "- Compute, for each community, the **personalized pagerank** of their **influencers**, where the source nodes belong to the community **artificial intelligence**.\n",
    "- Hint: Useful function: `nx.pagerank()`; `personalization` parameter defines the probability of random jumping to a source node.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def personalized_ranking(G, nodes_community, communities, communities_count):\n",
    "    ''' input: G:nx.Graph \n",
    "               nodes_community:{node_id:community_id}\n",
    "               communities:[community_ids]\n",
    "               community_count:int\n",
    "       output: communities_ranking:{community_id:ranking}\n",
    "\n",
    "    '''\n",
    "    communities_ranking = default_ranking.copy()\n",
    "    influencers = community_influencers(G, nodes_community, communities, communities_count)\n",
    "\n",
    "    # Add your code here\n",
    "    \n",
    "    return communities_ranking\n",
    " \n",
    "personalized_ranking = personalized_ranking(G, nodes_community, communities, communities_count)\n",
    "sorted(personalized_ranking.items(), key=lambda x: x[1], reverse=True) if personalized_ranking != default_ranking else None # prints sorted results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 TF-IDF Community Ranking\n",
    "- Treat each community as a document.\n",
    "- Treat **artificial intelligence** as a query.\n",
    "- Rank the documents (communities) based on their similarity to the query.\n",
    "- Hint: Useful function: `cosine_similarity()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_ranking(G, nodes_community, communities, communities_count):\n",
    "    ''' input: G:nx.Graph \n",
    "               nodes_community:{node_id:community_id}\n",
    "               communities:[community_ids]\n",
    "               community_count:int\n",
    "       output: communities_ranking:{community_id:ranking}\n",
    "\n",
    "    '''\n",
    "    communities_ranking = default_ranking.copy()\n",
    "\n",
    "    documents = [''] * communities_count\n",
    "    query = ['artificial inteligence']\n",
    "    \n",
    "    for n in nodes_community:\n",
    "        labels = ''.join([str(e[2]['Label'] + ' ')*e[2]['Weight'] for e in G.edges(n, data=True)])\n",
    "        documents[communities.index(nodes_community[n])] += labels\n",
    "\n",
    "    # Add your code here\n",
    "    \n",
    "    return communities_ranking\n",
    "\n",
    "tfidf_ranking = tfidf_ranking(G, nodes_community, communities, communities_count)\n",
    "sorted(tfidf_ranking.items(), key=lambda x: x[1], reverse=True) if tfidf_ranking != default_ranking else None # prints sorted results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7 Rankings Correlation\n",
    "- Consider `personalized_ranking`, `tfidf_ranking` and `AVG(connectivity_ranking, tfidf_ranking)`.\n",
    "- Compute the `3x3` correlation matrix.\n",
    "- Discuss which rankings correlate more and why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_correlation(connectivity_ranking, personalized_ranking, tfidf_ranking):    \n",
    "    correlation = np.zeros([3,3])\n",
    "    \n",
    "    connectivity_ranking = [r[1] for r in sorted(connectivity_ranking.items(), key=lambda x: x[0])]\n",
    "    personalized_ranking = [r[1] for r in sorted(personalized_ranking.items(), key=lambda x: x[0])]\n",
    "    tfidf_ranking = [r[1] for r in sorted(tfidf_ranking.items(), key=lambda x: x[0])]\n",
    "    avg_connectivity_tfidf_ranking = [sum(x)/2 for x in zip(connectivity_ranking, tfidf_ranking)]\n",
    "    \n",
    "    # Add your code here    \n",
    "    \n",
    "    return correlation\n",
    "\n",
    "compute_correlation(connectivity_ranking, personalized_ranking, tfidf_ranking)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***ANSWER:***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 [Submit  your Notebook](https://moodle.epfl.ch/mod/quiz/view.php?id=1026302)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DIS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "name": "_merged",
  "toc": {
   "base_numbering": "0",
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Table of Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "0adb3f5f654606e41dbdfd3bbda90ddbd940f914bbda17aa2fe6222e3a406da4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
